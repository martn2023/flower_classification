{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a4a1a15-0979-458d-9cba-e364ce1d225a",
   "metadata": {},
   "source": [
    "# I. Regularized Linear Models\n",
    "We've discussed over-fitting concerns. One way to mitigate overfitting (a.k.a. regularize a model) is to trim down the number of involved inputs. At a high level, \"regularizing models\" means making predictive models more accurate through simplification i.e. __reducing error on training data by dropping redundant inputs__. These are the 3 models, but we'll save the math examples for later:\n",
    "\n",
    "##### Ridge\n",
    "Lowering the weights (coefficients) of the lowest impact variables\n",
    "\n",
    "##### Lasso\n",
    "Completely eliminating low-impact variables.\n",
    "\n",
    "##### Elastic Net\n",
    "See this as a combination of the  Ridge and Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f6c0c7-f81b-4c0c-8e3c-99db75972995",
   "metadata": {},
   "source": [
    "# II. Complex vs. Simple Models\n",
    "\n",
    "### Complex models\n",
    "- More inputs\n",
    "- Low \"bias\"\n",
    "- Overfitting\n",
    "- Low error on training data, but higher error on testing data\n",
    "- In other words, __has trouble__ creating overarching themes that you can apply to unseen data\n",
    "\n",
    "### Simple models\n",
    "- Less inputs\n",
    "- High \"bias\"\n",
    "- More error on training data, but more accurate on testing data\n",
    "- In other words, __better__ at creating overarching themes that you can apply to unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac94d08-d96f-4751-bcc0-5ceab38de89a",
   "metadata": {},
   "source": [
    "# III. Illustrative Example\n",
    "\n",
    "### Real-Life Scenario\n",
    "I was on a web forum for CPA candidates and you wanted to predict who is going to pass their licensing exams. Forum members were using their online practice test scores a week before their official exam dates as a wet finger in the air to guage whether they'd pass or not. Assuming the following data points were available, I wondered whether it would be help to incorporate additional factors into a model:\n",
    "\n",
    "- How many hours they logged in the online testing platform\n",
    "- Quantity of practice questioned they attempted to answer (correct or not)\n",
    "- The prestiege of their undergrad college\n",
    "- GPA\n",
    "- Years of industry experience\n",
    "\n",
    "### Example Reasons Why This is Flawed\n",
    "- The number of questions attempted is probably mirrored to number of hours logged\n",
    "- More experience might mean a better command of certain topics, so you would assume a higher probability of passing. However, people further out from school may have grown rustier when it comes to academic challenges because they've *\"been out of school too long\"* and lost their edge. Others may have grown complacent *\"I already got the jobs I wanted, no need for a CPA\"*\n",
    "- GPA doesn't match school 1:1 i.e. going to a more prestigious school may lower your GPA because it has a more intense curriculum, a higher bar set by the professor, or a less generous curve due to more competitive classmates\n",
    "\n",
    "Bottomline: we shouldn't try to cram in every data point we can find."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf39506-3a62-456c-8ec1-7f2ab8156bdf",
   "metadata": {},
   "source": [
    "# III. Math Explanations and Alpha\n",
    "One hyperparameter we set before the experiment is __\"alpha\"__. A higher alpha means we are more aggressive about shrinking (in Ridge)/eliminating (in Lasso) input variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1755d4-5fc7-47ed-913a-452a68a4a802",
   "metadata": {},
   "source": [
    "### 1. Ridge Regression\n",
    "Mechnically, this method uses quadratic values for training data errors. Conceptually, this is best for shrinking the value of variables, under the assumption that most/all of your variables are useful.\n",
    "\n",
    "Assume a proposed model is 1(x) + 2(y) + 10(z)\n",
    "\n",
    "__Ridge__ penalty would be 1 + 4 + 100 = 105. Coefficient C is reponsible for 100/105 = 95.2% of the penalty. However, that doesn't translate to a propotional shrinkage. It's the other way around: the A and B coefficients will probably have disproportionately larger reductions because they aren't contributing as much.\n",
    "\n",
    "The new model might be 0.8(x) + 1.6(y) + 9(z).\n",
    "Coefficients A and B ate a 20% reduction and C only had a 10% reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b494fb3b-9436-48de-8d9d-62a95a64bd60",
   "metadata": {},
   "source": [
    "### 2. Lasso Regression\n",
    "Mechanically, this method uses absolute values for training data errors. Conceptually, this about removing useless variables completely, by setting a __threshold__ parameter and wiping out any variables that fall below this threshold.\n",
    "\n",
    "Assume our previously proposed model of 1(x) + 2(y) + 10(z)\n",
    "\n",
    "If the threshold paramter is set to 3, then x and y will be wiped because the A and B coefficients are both below 3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee651791-c9e6-4207-a980-736d41e94814",
   "metadata": {},
   "source": [
    "### 3. Elastic Regression\n",
    "Keeping this [video](https://www.youtube.com/watch?v=1dKRdX9bfIo&ab_channel=StatQuestwithJoshStarmer) here as a placeholder for something that got me halfway:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4d91a8-a820-425d-95c8-2836c34173b7",
   "metadata": {},
   "source": [
    "# III. Early Stoppage\n",
    "\n",
    "### What is it?\n",
    "This is not a predictive model. Rather, this is a method to use the aforementioned predictive models better. We talked about Gradient Descent as an iterative way to find a minimum error (or maximum value).\n",
    "\n",
    "Early Stoppage means we basically stop iterating when we've reached a point of diminishing returns:\n",
    "- the model is not getting more accurate with each subsquent calculation OR\n",
    "- even worse, the model is starting to get worse\n",
    "\n",
    "### Why is it important?\n",
    "The core benefit is one of accuracy - quitting while you're ahead\n",
    "\n",
    "However, a secondary benefit is reduced computational load on your device's processing power and memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ac3f5a-2381-4807-b417-191a2471dd83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
